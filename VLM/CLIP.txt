
**CLIP (Contrastive Language-Image Pretraining)** is a model that maps both text and images into a shared embedding space, allowing direct comparison between the two. An **embedding** is a numerical vector (like a 512-dimensional array) that captures the semantic meaning of input data, whether it's a sentence or an image. To create these embeddings, the input text is first broken into **tokens**—numerical IDs representing each word or subword. For example, in the phrase `"a donut"`, the token `320` might represent `"a "` (including the space), and `18471` might represent `"donut"`. These tokenized inputs are then transformed into embeddings. By aligning semantically related items close together in this space, CLIP enables tasks like image retrieval or zero-shot classification. To measure how similar two embeddings are, **cosine similarity** is used—it evaluates the angle between vectors, ignoring their length. Think of it as a similarity score: **+1** means the items are highly related—like *"donut"* and *"muffin"*, which are visually and semantically similar. A score around **0** means no real relation—like *"donut"* and *"airplane"*, which share almost nothing in common. And **–1** would mean complete opposites in meaning or appearance. This makes cosine similarity ideal for helping CLIP separate matching from non-matching pairs using contrastive learning. Padding during tokenization ensures uniform input size, and the shared space allows comparisons without needing modality-specific shape information.

